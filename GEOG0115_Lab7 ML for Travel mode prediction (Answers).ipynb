{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mining Social and Geographic Datasets\n",
    "-----------------------------------\n",
    "\n",
    "GEOG0115 Computer Lab Week 7 \n",
    "-------------------------------\n",
    "\n",
    "Note: Notebook might contain scripts and instructions adapted from GEOG0115, GEOG0051. \n",
    "Contributors: Stephen Law, Michal Iliev, Mateo Neira, Nikki Tanu, Thomas Keel, Gong Jie, Jason Tang and Demin Hu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Overview of Content in this Jupyter Notebook\n",
    "===============\n",
    "> ### Lab Notebook 7.1: Machine Learning for travel mode prediction (tabular data)\n",
    "> ### Lab Exercise 7.1: XGBoost (Gradient Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab Notebook 7.1: Machine Learning for travel mode prediction (tabular data)\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week's notebook, we will be reviewing some of the concepts and algorithms we learnt in previous three classes such as <b>logistic regression</b>, <b>decision tree</b>, <b>Random Forest</b> and <b>Gradient Boosting</b> for the travel mode classification problem. Think of this as a **review notebook** to put some of what you learn into practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Application: Travel mode classification\n",
    "\n",
    "An important task in urban analytics and transport planning is to predict the travel mode choice of individuals. The aim of the task is to predict which travel mode an individual takes namely `[car,bike,walk]`. This type of research can be conducted using  `trajectory` data but also travel survey data that is based on various socio-economic, climatic, individual and neighbourhood attributes such as `['distance', 'density', 'age', 'male', 'ethnicity','education', 'income', 'cars', 'license', 'bicycles', 'weekend','diversity', 'green', 'temp', 'precip', 'wind']`. \n",
    "\n",
    "The primary data source for this study is the Dutch national travel survey (NTS) conducted from 2010 to 2012 based on individual travel diaries. The survey participants were asked to record every trip in six days randomly selected over a year to capture seasonal effects. Below is the description of the input variables. Please read **(Hagenauera et al. 2017)** as part of this lab seminar. \n",
    "\n",
    "**Trip**\t<br>\n",
    "`distance`\tTotal trip distance in km <br>\n",
    "`weekend`\tTrip is done at the weekend <br>\n",
    "`mode`\tMain travel mode (walk, bike, pt, car). pt refers to public transport. <br>\n",
    "**Individual**\t<br>\n",
    "`age`\tAge of participant in years <br>\n",
    "`education`\tEducation of participant (lower, middle, higher)<br>\n",
    "`ethnicity`\tEthnicity of participant (native, western, other)<br>\n",
    "`license`\tParticipant owns a driverâ€™s license (yes, no)<br>\n",
    "`male`\tMale participant (yes, no)<br>\n",
    "**Household**\t<br>\n",
    "`bicycles`\tNumber of bicycles per household<br>\n",
    "`cars`\tNumber of cars per household<br>\n",
    "`income`\tNet annual household income in 1,000â‚¬ (<20,â‰¥ 20â€“40,â‰¥ 40)<br>\n",
    "**Build and natural environment**\t<br>\n",
    "`density`\tAddress density, aggregated over post codes, in 1,000addresses per km2<br>\n",
    "`diversity`\tShannon diversity index of land use classes<br>\n",
    "`green`\tProportion of green space per post code area in %<br>\n",
    "**Weather**\t<br>\n",
    "`precip`\tDaily precipitation sum in mm<br>\n",
    "`temp`\tDaily maximum temperature in Â°C<br>\n",
    "`wind`\tDaily average wind speed in m/s<br>\n",
    "\n",
    "\n",
    "\n",
    "[1] Hagenauera,J., Helbich,M. (2017) Comparative study of machine learning classifiers for modelling travel mode choice. Expert Systems with Applications (78)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Scikit-learn\n",
    "You may notice below that we import only certain portions of the ```sklearn``` package, and this is so as to conserve memory on your operating system through importing just what we will use. Remember also that what we are introduced to in these notebooks are but a drop in the ocean of possibilities of how you could implement Machine Learning principles with Python, and so, as your interests guide you, feel free to explore content and functions beyond what is set out here and in addition to some the links for further reading placed in these notebooks.\n",
    "\n",
    "For more infomation, look at [the official website](https://scikit-learn.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.1 Supervised Learning - Logistic Regression\n",
    "\n",
    "### Logistic Regression (Baseline Model)\n",
    "\n",
    "Let's start with running a logistic regression as the baseline classifier. As a reminder, a <b>Logistic Regression model</b> is a variant of a generalised linear models where a <b>sigmoid</b>  function $s(x)=\\frac{1}{1+e^{-f(x)}}$ is typically used in translating a standard linear regression into a binary classification problem $y_i\\in{0,1}$ which can be extended into a multi-class classification problem $y_i\\in{1,...,k}$. While linear regression models predict an output that is a continuous variable, logistic regression models are instead used for classification with a discrete categorical outcome (E.g. whether the flower is an Iris or rather an individual takes a bus to school). These models predict the <b> probability</b> of a certain event or categorisation happening: based on the variables defined by the user, each observation will be given a probability of between 0 and 1 on the output (event/class) in happening. Thereafter, the observations can be classified into 2 or more classes based on user-defined thresholds. \n",
    "\n",
    "Note: The logistic regression function in ```sklearn``` implements regularized logistic regression to give a predicted output of 2 or more classes. Read more about it [here](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) and [here](https://mlu-explain.github.io/logistic-regression/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's read in the national travel survey data\n",
    "dft=pd.read_csv('Travel_Mode_Classification/nts_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcols=['distance', 'density', 'age', 'male', 'ethnicity',\n",
    "       'education', 'income', 'cars', 'license', 'bicycles', 'weekend',\n",
    "       'diversity', 'green', 'temp', 'precip', 'wind']\n",
    "ycols=['mode_main']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the x-variables (input) and y-variable (outcome of classification)\n",
    "X = dft[Xcols]\n",
    "y = dft[ycols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's first look at the dependent variable (categorical) of interests using a barplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y.value_counts()).plot(kind='barh',color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ¤¨ TASK\n",
    "what do you notice? are the different classes balance? and which is the most popular mode of transport and which is the less?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's now take a look at the independent variables which has a total of 230,607 samples, there seems to be both numeric and categorical variables in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing the dataset\n",
    " \n",
    "Before building a classifier, it is important to **understand** and **describe** the data contained within the imported data frames. For example, how is the data distributed? Are there any missing values?\n",
    "\n",
    "For numeric variables\n",
    "* descriptive statistics\n",
    "* histograms\n",
    "\n",
    "For categorical variables\n",
    "* bar-plot\n",
    "* counts\n",
    "\n",
    "Beyond that, depending on your application, it is also common to look at how the variables correlate with each other using a correlation matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's check whether there are any missing data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_count = X.isna().sum()\n",
    "print(\"Count of missing values\")\n",
    "print(missing_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's then describe the data statistically. What was the mean/median `temp` and `percip` for example in the dataset? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **numeric variables**, you can visualise the distribution of the data using histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data=X.select_dtypes(include=[np.number])\n",
    "numeric_data.hist(figsize=(10,10),color='black',grid=False)\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you might notice most of the variables distributions are uni-modal, some are symmetrical while a couple are highly skewed. Do the distribution makes sense? Based on our previous learning, `distance` (distance travel) is highly skewed meaning the majority of the trips are shorter in distance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's then describe the categorical variable using barplots which counts the frequency for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_data = X.select_dtypes(include=[object])\n",
    "categorical_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(2,3,figsize=(20,10))\n",
    "categorical_data['male'].value_counts().plot(kind='bar',color='black',ax=ax[0,0])\n",
    "categorical_data['ethnicity'].value_counts().plot(kind='bar',color='black',ax=ax[0,1])\n",
    "categorical_data['education'].value_counts().plot(kind='bar',color='black',ax=ax[0,2])\n",
    "categorical_data['income'].value_counts().plot(kind='bar',color='black',ax=ax[1,0])\n",
    "categorical_data['license'].value_counts().plot(kind='bar',color='black',ax=ax[1,1])\n",
    "categorical_data['weekend'].value_counts().plot(kind='bar',color='black',ax=ax[1,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you might notice `male`,`education`,`income` has a more balanced class than `ethnicity`,`license` and `weekend`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "Before we can use the data in our model we always need to transform it, process it, standardise it, clean it, etc. \n",
    "\n",
    "As a reminder, a couple of common data preparation procedures including;\n",
    "1. coding categorical variables into numeric variables\n",
    "2. transforming the numerous variables to be more symmetrically distributed (it is not an assumption of machine learning models but in practice it often helps in training)\n",
    "3. standardise the numerous variables to make the features more comparable and to improve stability in training.\n",
    "\n",
    "Please read more [here](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now transform the highly skewed numeric variables so that it is less skewed. This is not a necessary assumption for most machine learning algorithm but it helps in practice. It also reduces the influence of the extreme values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "#Â you can calculate the skewness of the variable by using skew function in scipy.stats\n",
    "numeric_data=X.select_dtypes(include=[np.number])\n",
    "\n",
    "skewed = X[numeric_data.columns].apply(lambda x: skew(x.dropna().astype(float)))\n",
    "\n",
    "# these are all the variables that are very positively skewed. \n",
    "rskewed = skewed[(skewed > 1)].index\n",
    "\n",
    "# these are all the variables that are very negatively skewed. \n",
    "lskewed = skewed[(skewed < -1)].index\n",
    "print (f'leftskewed:{lskewed} and right-skewed:{rskewed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see a number of variables are highly right-skewed so let's transform these ones specifically. For right-skewed variables, log-transform the variable `X[rskewed] = np.log1p(X[rskewed])`. If there were any left-skewed variables, you would log transform the reflected skewed variable `X[lskewed] = np.log1p(np.max(X[lskewed])-X[lskewed])`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before transformation plot\n",
    "X[rskewed].hist(bins=20,figsize=(15,7), color='skyblue', xlabelsize=0, ylabelsize=0, grid=False, layout=(2,5))\n",
    "plt.show()\n",
    "\n",
    "# log-transforms the right skewed reflected variables of the dataset \n",
    "X[rskewed] = np.log1p(X[rskewed])\n",
    "\n",
    "# now plot again the variables after their log transformation\n",
    "X[rskewed].hist(bins=20,figsize=(15,7), color='orange', xlabelsize=0, ylabelsize=0, grid=False, layout=(2,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reminder note on transformation\n",
    "\n",
    "What do you notice different pre- and post-transformation for the variables above? For which variables does the log-transformation seem to make it less skewed?  \n",
    "\n",
    "It is important to note, data transformations is common to make skewed variable less skewed but it must be applied very cautiously and it isn't a necessary condition for running regression models and you loses interpretability for the specific feature. It's crucial to examine the distribution of your data and consider the context of your analysis before deciding whether to log transform the numeric variables. Additionally, if you do choose to log transform, ensure that you handle zero values appropriately (e.g., by adding a small constant before transformation if zero values are present).\n",
    "\n",
    "Be sure to play around with the many different arguments that allow you to customise the look of your multi-grid histogram plots with the `.hist()` (and all the other functions you come to interface with). \n",
    "\n",
    "Other more advance forms of transforms (`power transform`) are detailed [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.power_transform.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations/Covariance between variables\n",
    "\n",
    "We can use a correlation matrix to understand the association between variables. Variables are almost always correlated. Ideally, we want our input variables to not be totally correlated with one another. Significant correlation between our independent variables undermines the robustness of our model and interpretability.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create correlation matrix to understand the association between variables\n",
    "import seaborn as sns\n",
    "corr = X[numeric_data.columns].corr()\n",
    "f, ax = plt.subplots(figsize=(15, 15))\n",
    "sns.heatmap(corr, center=0,cmap=plt.get_cmap('magma_r'),\n",
    "            square=True, linewidths=.05, annot=True, vmin=-1, vmax=1,ax=ax) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ¤¨ TASK\n",
    "What do you notice? Which variables are highly correlated with each other and which are not? \n",
    "Why do you think `density` and `green` have a strong negative correlation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we learnt in the previous classes, there are many different methods to explore the data. `Seaborn` is a popular library to further interrogate the data. More information can be found [here](https://seaborn.pydata.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardise data\n",
    "We need to standardise the data (again!) to make them comparable and more stable for training. Remember the units also become less interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[numeric_data.columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line creates an instance of the Standard Scaler to transform the data\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this fits and transforms the data\n",
    "X[numeric_data.columns] = scaler.fit_transform(X[numeric_data.columns]) #we standardise all variables, including the log transformed ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[numeric_data.columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's then cast the categorical variables into numeric variables for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.concat([X,pd.get_dummies(X['ethnicity'])],axis=1)\n",
    "X=pd.concat([X,pd.get_dummies(X['male'])],axis=1)\n",
    "X=pd.concat([X,pd.get_dummies(X['education'])],axis=1)\n",
    "X=pd.concat([X,pd.get_dummies(X['income'])],axis=1)\n",
    "X=pd.concat([X,pd.get_dummies(X['license'])],axis=1)\n",
    "X=pd.concat([X,pd.get_dummies(X['weekend'])],axis=1)\n",
    "\n",
    "#The pd.concat() operation with pd.get_dummies() adds new columns to X (dummy variables) \n",
    "#and 'X.columns=' ensures that those columns have the desired labels\n",
    "\n",
    "X.columns=['distance', 'density', 'age', 'male', 'ethnicity', 'education',\n",
    "       'income', 'cars', 'license', 'bicycles', 'weekend', 'diversity',\n",
    "       'green', 'temp', 'precip', 'wind', 'native_eth', 'nonwestern_eth', 'western_eth',\n",
    "       'no_male', 'yes_male', 'higher_edu', 'lower_edu', 'middle_edu', '20to40_inc', 'less20_inc', 'more40_inc',\n",
    "       'no_license', 'yes_license', 'no_weekend', 'yes_weekend']\n",
    "\n",
    "#it is redundant to keep both 'no_license' and 'yes_license' etc. so we select only specific variables\n",
    "X=X[['distance', 'density', 'age', 'cars', 'bicycles', 'diversity',\n",
    "       'green', 'temp', 'precip', 'wind', 'native_eth', 'nonwestern_eth', 'western_eth',\n",
    "       'yes_male', 'higher_edu', 'lower_edu', 'middle_edu','20to40_inc', 'less20_inc', 'more40_inc',\n",
    "       'yes_license', 'yes_weekend']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for the target variable (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "Label = LabelEncoder() #easy labelling with LabelEncoder\n",
    "y=pd.concat([y,pd.DataFrame(Label.fit_transform(y),columns=['labels'])],axis=1)\n",
    "ylabels=y['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into training and test groups\n",
    "\n",
    "As we mentioned in the last couple of classes, in machine learning applications, it is **important** to split the entire dataset at least two sets - the `_train` and `_test` sets to see how well it generalises - i.e., is it over-fitted to the dataset you made it using, such that it has little worth when used to predict outcomes on another dataset? In many cases, you might need to have three dataset namely `train/val/test` where the validation dataset in this case would be only used for the purposes of hyper-parameter tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splits the datasets (both x- and y-variables) into the training and test sets\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(X, ylabels, train_size=0.7, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a new logistic regression model\n",
    "model = LogisticRegression()\n",
    "#fits (/configures) this model using the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#then tries to make a prediction using the test dataset's x-variables\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ¤¨ TASK\n",
    "**Always sanity check** : do the results make sense? How do these predictions compare to the ground-truth? Plot the first 10 samples in y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the logistic regression estimates **probabilities** (in this case, which travel mode an individual takes), then classifies them into the mode they most likely have taken. To have an understanding of how accurate our model was, we can, instead of asking the model to give us the final classifications (travel mode), return the probabilities that it calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict probabilities of allocation of each categories\n",
    "y_pred_prob=model.predict_proba(X_test)\n",
    "pd.DataFrame(y_pred_prob,columns=[y.mode_main.unique()]).head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting Results - Classificaton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's report the result with accuracy (the number of correctly predicted divided by its total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reports the prediction accuracy of the testset\n",
    "print ('the out of sample test accuracy is : '+ str(round(accuracy_score(y_test, y_pred),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compare the results to the training set prediction. A significant difference between training and test accuracy (e.g., high training accuracy (below) but low test accuracy (above)) would indicate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reports the prediction accuracy of the trainset\n",
    "y_pred_train=model.predict(X_train) \n",
    "print ('the in sample test accuracy is : '+ str(round(accuracy_score(y_train, y_pred_train),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are really similar meaning the model is likely not to be overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, for more details of the model, we can use the ```classification_report``` function in ```sklearn```. The report shows some key metrics through which we can judge the accuracy of the logistic regression model we have created: **accuracy,precision, recall and f1-score**. The calculation of precision and recall statistics entails using measures such as the rates of true and false negatives/positives (by comparing the predicted and observed) while F1 score is a composite measure that takes into consideration both the former two. In general these measures should be reported specially when the data is imbalanced (as here). \n",
    "\n",
    "More formally;\n",
    "\n",
    "$Precision = TP/(TP+FP)$\n",
    "\n",
    "$Recall = TP/(TP+FN)$\n",
    "\n",
    "$F1 = 2(Precision*Recall)/(Precision+Recall)$\n",
    "\n",
    "Read more about these statistics [here](https://muthu.co/understanding-the-classification-report-in-sklearn/) or look at documentation [here](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report). \n",
    "\n",
    "```classification_report``` provides the results for each category as well as the macro averages (i.e. averages across the categories).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "Let's also produce the <b>confusion matrix</b>, which shows exactly what proportion of each travel choice was rightly and wrongly classified. The x- and y-axes of the plot below shows, respectively, the travel mode that was predicted (```Predicted label```) vs the ground truth (```True label```) is. Those along the diagonals are rightly predicted and those along the off diagonals are wrongly predicted or confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ¤¨ TASK\n",
    "what do you notice? which travel mode is making the most errors? why do you think is the case here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model comparison - Ensemble Learners\n",
    "Comparing models is a critical aspect of machine learning. What makes using the syntax of `sklearn` so convenient is that you only need to make a few adjustments to run a different model ie. `model.fit`. In this case we will learn about `Decision Tree` Classifiers and `Random Forest` Classifiers.\n",
    "\n",
    "Let's take this opportunity to try out a different model as an example. In this case we will use the `RandomForestClassifier`. `RF` is an `ensemble learning` method which uses multiple simple predictors in order to make a better prediction. The hypothesis is that multipler learners will generate better results than a single learner. For tabular data, `ensemble learning` algorithms are even outperforming deep neural networks such as transformers (Shwartz-Ziv et al 2022).\n",
    "\n",
    "In greater detail, **Random Forest Regressors/ Classifiers** (Breiman 2001), is based on fitting a multitude of [Decision Trees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)[1], each on samples drawn with replacement (i.e., bootstrap sample) from the training set. The outcomes from multiple simple models across multiple observations are often i. the one that is most often predicted (majority voting) across multiple trees for classification or ii. the mean or average prediction across multiple trees for regression.\n",
    "\n",
    "And lastly, another ensemble learning method is **Gradient Boosting Regressors/ Classifiers** (Friedman 2001), which combines many weak \"learners\" into a single strong learner in an iterative fashion where each regressor tries to improve the previous model. The one implemented in `sklearn` is called `GradientBoostingClassifier`. \n",
    "\n",
    "There are many more machine learning algorithms and this is only the tip of the ice-berg. We will explore one in today's exericise. The aim here isn't to describe every one of these but rather to show the spectrum of methods where the student can then go and explore.  \n",
    "\n",
    "<img src=\"Random Forest Diagram (wikipedia-commons).png\" alt=\"drawing\" width=\"620\" align='center' >\n",
    "<center><b>Fig 1. Random Forest Diagram (wikipedia-commons).</b></center>\n",
    "\n",
    "\n",
    "[1] It is important to give a simple description of Decision Tree here. DT is another supervised learning algorithm that learns simple decision rules inferred from the data features. As implied by the 'tree' in its name, the model can pass each observation that goes through it via multiple branches (or routes) of decisions to help it derive a final outcome for that given observation). [For a premier of Decision Trees in plain English, this](https://towardsdatascience.com/decision-trees-explained-3ec41632ceb6) is a good link to have a brief read at! \n",
    "\n",
    "Breiman, L. (2001). Random forests. Machine learning, 45(1), 5-32. \n",
    "\n",
    "Friedman, J.(2001). Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, 29(5)\n",
    "\n",
    "Shwartz-Ziv, R., & Armon, A. (2022). Tabular data: Deep learning is not all you need. Information Fusion, 81, 84-90.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_forest =  RandomForestClassifier() \n",
    "model_forest.fit(X_train, y_train)\n",
    "y_pred=model_forest.predict(X_test)\n",
    "print ('the out of sample test accuracy for a RandomForestClassifier is : '+ str(round(accuracy_score(y_test, y_pred),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the calculation takes longer, but the accuracy is higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter tuning and Cross Validation\n",
    "\n",
    "Many models in machine learning have hyper-parameters which are parameters that are not directly learnt within the estimators but instead needs to be tuned. Typical hyper-parameters include `max_depth`(The maximum depth of the tree) and `n_estimators`(The number of trees in the forest) for [RandomForestClassifiers](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). Let's set randomly here `max_depth`=20. Remember to read the documentation of the model to know what each of these hyper-parameters mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_forest2 =  RandomForestClassifier(max_depth=20) \n",
    "model_forest2.fit(X_train, y_train)\n",
    "y_pred=model_forest2.predict(X_test)\n",
    "print ('the out of sample test accuracy for a RandomForestClassifier is : '+ str(round(accuracy_score(y_test, y_pred),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the results better or worst? how do we know whether these parameters yield the best results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFold Cross Validation\n",
    "\n",
    "As mentioned previously, when training a machine learning model that includes hyper-parameters. It is important to split the training data into a validation set. However, partitioning the training data into a train and validation set drastically reduce the number of samples which can be used for learning the model, while the results can depend on the particular validation set. A useful and common resampling procedure is  k-fold Cross validation, the training set is split into k smaller folds, where a model is trained on each folds of the training data and validated on the remaining parts of the data. This is repeated until all folds are exhausted. The results are calculated on an out-of-sample testset.\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" alt=\"drawing\" width=\"420\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV\n",
    "\n",
    "It is possible and recommended to search the hyper-parameter space for the best cross validation score using cross validation. A simple way to do this is called the `GridSearchCV` which exhaustively generates candidates from a grid of parameter values specified . To reduce the number of possible models to test, we restrict the search where \n",
    "`max_depth`:[10,40] and `n_estimators`:[100]. \n",
    "You can read more about it from [here](https://scikit-learn.org/stable/modules/grid_search.html)\n",
    "and [here](https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d) for an overview of what the main parameters are, and how you could tune them.  <br/>\n",
    "\n",
    "**The process may take a while or doesn't work on your machine if doesn't have enough disk space. You are welcome to use [Google Colab online](https://colab.research.google.com/).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'max_depth': [10, 40],\n",
    "    'n_estimators': [100]#, 200, 300]# restricted the grid search to reduce compute time \n",
    "}\n",
    "# Create a based model\n",
    "model = RandomForestClassifier()# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = model, param_grid = param_grid, \n",
    "                          cv = 3 # number of folds for cross validation\n",
    "                           , n_jobs = -1 #Â use all processors\n",
    "                            , verbose = 2 # what to display\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note, `GridSearchCV` is applied only on the training set here when you create three folds. Remember you can also just simply split the training set into a training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid_search.best_estimator_\n",
    "y_pred=model.predict(X_test)\n",
    "print ('the out of sample test accuracy for a RandomForestClassifier is : '+ str(round(accuracy_score(y_test, y_pred),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ¤¨ TASK\n",
    "Try grid searching for more hyper-parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification accuracy will likely have improved and even if it hasn't improve, we can say that the results are robust to different settings of hyper-parameters. \n",
    "You might have also noticed, due to this process of hyper-parameter tuning, it takes a lot longer to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "Due to the black-box nature of most machine learning algorithms, there are no easy way to interpret similar coefficients. However, there are **interpretable ML** approaches to measure feature importance for methods like `Random Forest (RF)`. A **model-agnostic** way to interpret a machine learning model such as `RF` is [permutation importance](https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance).\n",
    "\n",
    "**The process may take a while or doesn't work on your machine if it doesn't have enough disk space. You are welcome to use [Google Colab online](https://colab.research.google.com/).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "# if it doesn't work,you meight need to change n_jobs\n",
    "result = permutation_importance(\n",
    "    model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "coef = pd.DataFrame(result.importances_mean,index=X.columns,columns=['importance'])\n",
    "coef = coef.reset_index()\n",
    "coef.columns=['features','importance']\n",
    "coef['importance']=np.abs(coef['importance'])\n",
    "coef=coef.sort_values(by='importance',ascending=False)\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "sns.barplot(x='importance',y='features',data=coef,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab Exercise 7.1 XGBoost \n",
    "-------------------------------\n",
    "\n",
    "For this week's exercise, we will have the student explore a popular **gradient boosting** [1] algorithm implementation called **XGBoost** (Extreme Gradient Boosting), which is a type of ensemble gradient boosting algorithm where weak learners (simple models) are built sequentially to correct the errors made by the preceding models. In simple terms, gradient boosting focuses on reducing the residual errors iteratively. `sklearn` has an implementation of the algorithm called `gradientboosting` but the `xgboost` version is much quicker. \n",
    "\n",
    "You can find the reference for the algorithm with this link [2] and the python library here [3]. The library can be easily installed with pip or conda. \n",
    "\n",
    "`!pip install xgboost`\n",
    "\n",
    "[1] Friedman, J.(2001). Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, 29(5)\n",
    "\n",
    "[2] Chen, T., & Guestrin, C. (2016). Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining (pp. 785-794).\n",
    "\n",
    "[3] xgboost. https://xgboost.readthedocs.io/en/stable/#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, please run a similar **predictions** using the `XGBoost` library. Remember this is a multi-class classification problem and not a regression problem so please use the appropriate method. Remember to tune the hyper-parameters of the classifier. For `xgboost` an important parameter to tune is `eta/learning rate` which shrinks the contribution of each tree to help prevent overfitting and improve generalization. \n",
    "[link](https://xgboost.readthedocs.io/en/stable/parameter.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Prepare the data for XGBoost using the DMatrix class, which optimizes memory usage and computation\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)  \n",
    "dtest = xgb.DMatrix(X_test, label=y_test)    \n",
    "\n",
    "# Set up parameters for XGBoost model training\n",
    "params = {\n",
    "    'max_depth': 3,               # Maximum depth of a tree to control model complexity and prevent overfitting\n",
    "    'eta': 0.1,                   # Learning rate (step size); try adjusting this value for better convergence\n",
    "    'objective': 'multi:softmax', # Specifies the loss function for multi-class classification\n",
    "    'num_class': 4,               # Number of unique classes in the target variable\n",
    "}\n",
    "\n",
    "num_boost_round = 20 # Number of boosting rounds (iterations); higher values can improve model performance but may increase training time\n",
    "\n",
    "# Train the model using the training data and defined parameters\n",
    "bst = xgb.train(params, dtrain, num_boost_round)\n",
    "\n",
    "# Make predictions on the test set using the trained model\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "# Evaluate the model's accuracy using the actual labels and predictions\n",
    "accuracy = accuracy_score(y_test, preds) \n",
    "print(\"Accuracy: {:.2%}\".format(accuracy))  # Print accuracy as a percentage with 2 decimal places\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ¤¨ TASK\n",
    "\n",
    "It might be a good time now to read the following article and try to replicate some of the experiments of the paper.\n",
    "\n",
    "[1] Hagenauera,J.& Helbich,M. (2017) Comparative study of machine learning classifiers for modelling travel mode choice. Expert Systems with Applications (78)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
